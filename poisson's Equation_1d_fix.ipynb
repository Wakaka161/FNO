{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b60978cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from d2l import torch as d2l\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "from timeit import default_timer\n",
    "from utilities3 import *\n",
    "\n",
    "from Adam import Adam\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae12e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#  1d fourier layer\n",
    "################################################################\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1):\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1  # Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(\n",
    "            self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        # Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1) // 2 + 1, device=x.device, dtype=torch.cfloat)\n",
    "        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], self.weights1)\n",
    "        # Return to physical space\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        return x\n",
    "\n",
    "class FNO1d_block(nn.Module):\n",
    "    '''FNO块，其中未实现激活函数'''\n",
    "    def __init__(self, modes, width):\n",
    "        super().__init__()\n",
    "        self.modes1 = modes\n",
    "        self.width = width\n",
    "        self.conv0 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        return x\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.dim = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(dims=self.dim)\n",
    "        return x\n",
    "\n",
    "class Get_grid(nn.Module):\n",
    "    def __init(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x = shape[0], shape[1]\n",
    "        gridx = torch.tensor(np.linspace(0, 2 * math.pi, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1).repeat([batchsize, 1, 1])\n",
    "        return gridx.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        grid = self.get_grid(x.shape, x.device)\n",
    "        x = torch.cat((x, grid), dim=-1)\n",
    "        return x\n",
    "\n",
    "# class convd_0(nn.Module):\n",
    "#     def __init__(self, out_channels, num_padding, num_kernel):\n",
    "#         super().__init__()\n",
    "#         self.out_channels = out_channels\n",
    "#         self.num_padding = num_padding\n",
    "#         self.num_kernel = num_kernel\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         if x.shape[1] == 2:\n",
    "#             x = F.pad(x, self.num_padding).unsqueeze(dim=1)\n",
    "#         else:\n",
    "#             x = F.pad(x, self.num_padding).unsqueeze(dim=2)\n",
    "# #         elif len(x.shape) == 4:\n",
    "# #             x = F.pad(x, num_padding)\n",
    "#         convd_layer = nn.Conv2d(in_channels=x.shape[1], out_channels=self.out_channels, \n",
    "#                                 kernel_size=self.num_kernel, device=x.device)\n",
    "#         convd_out = convd_layer(x)\n",
    "#         if convd_out.shape[2] == 1:\n",
    "#             convd_out = convd_out.squeeze(dim=2)\n",
    "#         return convd_out.to(x.device)\n",
    "\n",
    "#     def convd_1(self, x, out_channel, num_padding, num_kernel):\n",
    "#         if len(x.shape) == 3:\n",
    "#             x = F.pad(x, num_padding).unsqueeze(dim=2)\n",
    "#         elif len(x.shape) == 4:\n",
    "#             x = F.pad(x, num_padding)\n",
    "#         in_channel=x.shape[1]\n",
    "#         convd_layer = nn.Conv2d(in_channel, out_channel, kernel_size=num_kernel, device=x.device)\n",
    "#         convd_out = convd_layer(x)\n",
    "#         if convd_out.shape[2] == 1:\n",
    "#             convd_out = convd_out.squeeze(dim=2)\n",
    "#         return convd_out.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513f1339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 414337\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "#  configurations\n",
    "################################################################\n",
    "ntrain = 10000\n",
    "ntest = 1025\n",
    "\n",
    "sub = 2 ** 3  # subsampling rate\n",
    "h = 2 ** 13 // sub  # total grid size divided by the subsampling rate\n",
    "s = h\n",
    "\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "epochs = 10\n",
    "step_size = 50\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 16\n",
    "width = 64\n",
    "\n",
    "################################################################\n",
    "# read data\n",
    "################################################################\n",
    "\n",
    "# Data is of the shape (number of samples, grid size)\n",
    "dataloader = MatReader('../data/possion_data3.2_10000_1024.mat')\n",
    "x_train = dataloader.read_field('f_train')[:ntrain, :]\n",
    "y_train = dataloader.read_field('u_train')[:ntrain, :]\n",
    "x_test = dataloader.read_field('f_test')[:ntest, :]\n",
    "y_test = dataloader.read_field('u_test')[:ntest, :]\n",
    "\n",
    "x_train = x_train.reshape(ntrain, -1, 1)\n",
    "x_test = x_test.reshape(ntest, -1, 1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_train, y_train), batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "model=nn.Sequential(Get_grid(), nn.Linear(2, width), Permute(0,2,1),\n",
    "                    FNO1d_block(modes, width), nn.ReLU(),\n",
    "                    FNO1d_block(modes, width), nn.ReLU(),\n",
    "                    FNO1d_block(modes, width), nn.ReLU(),\n",
    "                    Permute(0,2,1), nn.Linear(width, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "model = model.cuda()\n",
    "model.apply(xavier)\n",
    "print('number of params:', count_params(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d346766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13.7066567 17146.20271066284 0.05830602565854788 0.03521356687313173\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), y\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m mse \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(out\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), y\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m l2 \u001b[38;5;241m=\u001b[39m myloss(out\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), y\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mE:\\anaconda_install\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\anaconda_install\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mE:\\anaconda_install\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mFNO1d_block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 46\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw0(x)\n\u001b[0;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m+\u001b[39m x2\n",
      "File \u001b[1;32mE:\\anaconda_install\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mSpectralConv1d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Multiply relevant Fourier modes\u001b[39;00m\n\u001b[0;32m     30\u001b[0m out_ft \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batchsize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcfloat)\n\u001b[1;32m---> 31\u001b[0m out_ft[:, :, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodes1] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompl_mul1d(x_ft[:, :, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodes1], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights1)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Return to physical space\u001b[39;00m\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mirfft(out_ft, n\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "# training and evaluation\n",
    "################################################################\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "myloss = LpLoss(size_average=False)\n",
    "for ep in range(epochs):\n",
    "    model.train()\n",
    "#     animator = Animator(xlabel='epoch', xlim=[1, epochs],legend=['train_l2'])\n",
    "    t1 = default_timer()\n",
    "    # train_mse = 0\n",
    "    # train_l2 = 0\n",
    "    metric_train = Accumulator(3) #累加每epoch，训练集上的mseloss,训练集上的l2_loss,测试集上的l2_loss\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device='cuda'), y.to(device='cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "\n",
    "        mse = F.mse_loss(out.view(batch_size, -1), y.view(batch_size, -1), reduction='mean')\n",
    "        l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1))\n",
    "        l2.backward() # use the l2 relative loss\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_train.add(mse.item(), l2.item(), 0)\n",
    "        # train_mse += mse.item()\n",
    "        # train_l2 += l2.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    # test_l2 = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "\n",
    "            out = model(x)\n",
    "            test_l2 = myloss(out.view(batch_size, -1), y.view(batch_size, -1)).item()\n",
    "            metric_train.add(0, 0, test_l2)\n",
    "\n",
    "    train_mse = metric_train[0] / len(train_loader)\n",
    "    train_l2 = metric_train[1] / ntrain\n",
    "    test_l2 = metric_train[2] / ntest\n",
    "\n",
    "    t2 = default_timer()\n",
    "    print(ep, t2-t1, train_mse, train_l2, test_l2)\n",
    "#     animator.add(ep + 1, train_l2)\n",
    "\n",
    "# torch.save(model.state_dict(), 'model/params_possion_data2.5_10000_1024_fix')\n",
    "\n",
    "# pred = torch.zeros(y_test.shape)\n",
    "# index = 0\n",
    "# test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(x_test, y_test), batch_size=1, shuffle=False)\n",
    "# with torch.no_grad():\n",
    "#     for x, y in test_loader:\n",
    "#         test_l2 = 0\n",
    "#         x, y = x.cuda(), y.cuda()\n",
    "#\n",
    "#         out = model(x).view(-1)\n",
    "#         pred[index] = out\n",
    "#\n",
    "#         test_l2 += myloss(out.view(1, -1), y.view(1, -1)).item()\n",
    "#         if index % 10 == 0:\n",
    "#             print(index, test_l2)\n",
    "#         index = index + 1\n",
    "\n",
    "# scipy.io.savemat('pred/burger_test.mat', mdict={'pred': pred.cpu().numpy()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "num=random.randint(1,ntrain)\n",
    "print(num)\n",
    "x_random=x_train[num,:,:].reshape(1,-1,1)\n",
    "y_random=y_train[num,:]\n",
    "y_pre=model(x_random.cuda())\n",
    "plt.plot(np.linspace(0,2*math.pi,len(y_random)), y_random, color=\"blue\", linewidth=3.0, linestyle=\"-\", label=\"-\")\n",
    "plt.plot(np.linspace(0,2*math.pi,len(y_random)), y_pre.view(-1).to(torch.device(\"cpu\")).detach().numpy(), color=\"red\", linewidth=1.0, linestyle=\"solid\", label=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6084b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf19cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
